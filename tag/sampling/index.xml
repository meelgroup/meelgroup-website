<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sampling | MeelGroup</title>
    <link>https://meelgroup.github.io/tag/sampling/</link>
      <atom:link href="https://meelgroup.github.io/tag/sampling/index.xml" rel="self" type="application/rss+xml" />
    <description>sampling</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019</copyright><lastBuildDate>Tue, 09 Jul 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://meelgroup.github.io/images/icon_hu1dd832c4da814f17fe02e3737f0ae144_14882_512x512_fill_lanczos_center_2.png</url>
      <title>sampling</title>
      <link>https://meelgroup.github.io/tag/sampling/</link>
    </image>
    
    <item>
      <title>Barbarik</title>
      <link>https://meelgroup.github.io/project/barbarik/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://meelgroup.github.io/project/barbarik/</guid>
      <description>&lt;p&gt;The divide between the existence of sampling techniques that have strong theoretical guarantees but fail to scale and scalable techniques with weak or no theoretical guarantees mirrors the gap in software engineering between poor scalability of classical program synthesis techniques and billions of programs that are routinely used by practitioners. One bridge connecting the two extremes in the context of software engineering has been program testing. In contrast to testing for deterministic programs, where one trace is sufficient to prove the existence of a bug, in case of samplers one sample is typically not sufficient to prove non-conformity of the sampler to the desired distribution. This makes one wonder whether it is possible to design testing methodology to test whether a sampler under test generates samples close to a given distribution.&lt;/p&gt;
&lt;p&gt;We present, Barbarik, to test whether the distribution generated is ε−close or η−far from the uniform distribution. In contrast to the sampling techniques that require an exponential or sub-exponential number of samples for sampler whose support can be represented by n bits, Barbarik requires only O(1/(η − ε)4) samples.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>WAPS</title>
      <link>https://meelgroup.github.io/project/waps/</link>
      <pubDate>Thu, 04 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://meelgroup.github.io/project/waps/</guid>
      <description>&lt;p&gt;Previous work on applying Knowledge compilation has focused on uniform sampling over all the variables. Since the constraints are written in high level languages such as Verilog, the popular CNF encoding schemes as Tseitin encoding introduces additional auxiliary variables. The resulting CNF formulas are not equivalent but equisatisfiable. In particular, for a formula $G$ specified in high level language we obtain a CNF formula F such that $G (X) = \exists Y F(X,Y)$. This makes one wonder if it is possible to extend Knowledge compilation based techniques to sample over a subset of variables. Furthermore, languages such as Verilog allow specification of weights to user-defined constraints, so there is a need to sample according to the posterior distribution. In this paper, we provide affirmative question to the above two questions: We propose KUS that samples over user defined subset of variables from posterior distribution for a given prior distribution defined over product spaces.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>KUS</title>
      <link>https://meelgroup.github.io/project/kus/</link>
      <pubDate>Wed, 01 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://meelgroup.github.io/project/kus/</guid>
      <description>&lt;p&gt;Uniform sampling has drawn diverse applications in programming languages and software engineering, like in constrained-random verification (CRV), constrained-fuzzing and bug synthesis. The effectiveness of these applications depend on the uniformity of test stimuli generated from a given set of constraints. Despite significant progress over the past few years, the performance of the state of the art techniques still falls short of those of heuristic methods employed in the industry which sacrifice either uniformity or scalability when generating stimuli. In this paper, we propose a new approach to the uniform generation that builds on recent progress in knowledge compilation. The primary contribution of this paper is marrying knowledge compilation with uniform sampling: our algorithm, KUS, employs the state-of-the-art knowledge compilers to first compile constraints into d-DNNF form, and then, generates samples by making two passes over the compiled representation. We show that KUS is able to significantly outperform existing state-of-the-art algorithms, SPUR and UniGen2, by up to 3 orders of magnitude in terms of runtime while achieving a geometric speedup of $1.7\times$ and $8.3\times$ over SPUR and UniGen2 respectively. Also, KUS achieves a lower PAR-2 score, around $0.82\times$ that of SPUR and $0.38\times$ that of UniGen2. Furthermore, KUS achieves speedups of up to 3 orders of magnitude for incremental sampling. The distribution generated by KUS is statistically indistinguishable from that generated by an ideal uniform sampler. Moreover, KUS is almost oblivious to the number of samples requested.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
