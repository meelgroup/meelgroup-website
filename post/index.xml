<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | MeelGroup</title>
    <link>https://meelgroup.github.io/post/</link>
      <atom:link href="https://meelgroup.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2019</copyright><lastBuildDate>Sun, 03 Nov 2019 00:00:00 +0700</lastBuildDate>
    <image>
      <url>https://meelgroup.github.io/images/icon_hu1dd832c4da814f17fe02e3737f0ae144_14882_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>https://meelgroup.github.io/post/</link>
    </image>
    
    <item>
      <title>CrystalBall: SAT solving, Data Gathering, and Machine Learning</title>
      <link>https://meelgroup.github.io/post/crystallball/</link>
      <pubDate>Sun, 03 Nov 2019 00:00:00 +0700</pubDate>
      <guid>https://meelgroup.github.io/post/crystallball/</guid>
      <description>&lt;p&gt; &lt;/i&gt;Link to the  &lt;a href=&#34;https://www.msoos.org/2019/06/crystalball-sat-solving-data-gathering-and-machine-learning/&#34;&gt;original post&lt;/a&gt; on CrystallBall.&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Knowledge Compilation meets Uniform Sampling</title>
      <link>https://meelgroup.github.io/post/kus/</link>
      <pubDate>Fri, 03 May 2019 11:06:17 +0530</pubDate>
      <guid>https://meelgroup.github.io/post/kus/</guid>
      <description>&lt;p&gt;This blogpost is based on our 
&lt;a href=&#34;https://www.comp.nus.edu.sg/~meel/Papers/lpar18.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; that got published in the procedings of International Conference on Logic for Programming, Artificial Intelligence and Reasoning (LPAR), 2018. The code is available 
&lt;a href=&#34;https://github.com/meelgroup/KUS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The primary contribution of this work is marrying knowledge compilation with uniform sampling to design a new uniform sampler KUS. The main result is that KUS is able to solve more number of benchmarks than existing state-of-the-art uniform and almost-uniform samplers beating them by orders of magnitude in terms of runtime:
&lt;img src=&#34;cactus.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt;
&lt;h3&gt;Uniform Sampling&lt;/h3&gt;
***
Given a boolean formula $F$, the idea of Uniform Sampling is to generate samples from the set of solutions of $F$ called $R_F$ using a generator $\mathcal{G}$ that guarantees:
$$\forall y \in R_F, \mathsf{Pr}\left[\mathcal{G}(F) = y\right] = \frac{1}{|R_F|},$$
Uniform sampling is a fundamental problem in computer science with wide range of applications ranging from bayesian analysis to software engineering and programming languages. Jerrum, Valiant, and Vazirani observed deep relationship between model counting and uniform sampling. In particular, they showed that given access to an exact model counter, one could design a uniform generator which requires only polynomially many queries to the exact model counter. On the other hand, knowledge compilation has been emerged as a vital task wherein a logical theory is compiled into a form that allows performing probabilistic inference in polynomial time. It is well known that there is a deep connection between probabilistic inference and model counting. In this context, one wonders if the recent advances in knowledge compilation can be harnessed to design a scalable uniform sampler. The primary contribution of this work is marrying knowledge compilation with uniform sampling to design a new algorithm, KUS, that performs uniform sampling, outperforming current state-of-the-art approximately uniform and uniform samplers.
&lt;h3&gt; Knowledge Compilation and d-DNNF representation&lt;/h3&gt;
***
To deal with computational intractability of probabilistic reasoning, knowledge compilation seeks to compile a knowledge base, often represented as a propositional formula in CNF, to a target language. Thereafter, probabilistic reasoning tasks, which are often expressed as sequence of queries, are performed by querying the knowledge base in the target language. Deterministic Decomposable Negation Normal Form (d-DNNF) have emerged as a central target language in knowledge compilation community since several probabilistic reasoning tasks such as probabilistic inference, maximum a posteriori (MAP) can be answered in polynomial time in the size of d-DNNF. A boolean formula in Negation Normal Form (NNF) is said to be in d-DNNF if it satisfes the following properties:
&lt;ul&gt;
&lt;li&gt; Deterministic: We refer to an NNF as deterministic if the operands of OR in all wellformed Boolean formula in the NNF are mutually inconsistent.&lt;/li&gt;
&lt;li&gt;Decomposable: We refer to an NNF as decomposable if the operands of AND in all wellformed Boolean formula in the NNF are expressed in a mutually disjoint set of variables.&lt;/li&gt;
&lt;/ul&gt;
![alt_text](ddnnf.png)
&lt;p&gt;d-DNNF of a boolean formula $F$ represent the set of satisfying assignment $R_F$&lt;/p&gt;
&lt;h3&gt;The algorithm&lt;/h3&gt;
***
The central idea behind KUS is to first employ the state-of-the-art knowledge compilation approaches to compile a given CNF formula into d-DNNF form, and then performing only two passes over the d-DNNF representation to generate as many identically and independently distributed samples as specified by the user denoted by $s$.
![alt_text](kus.png)
&lt;p&gt;KUS takes in a CNF formula $F$ and required number of samples s and returns a set of $s$ samples such that each sample is uniformly and independently drawn from the uniform distribution over the set of solutions $R_F$. KUS first invokes a d-DNNF compiler over the formula F to obtain its d-DNNF. Then, the subroutine Annotate is invoked that annotates d-DNNF by annotating each node with a tuple consisting of the number of solutions and the set of variables in the node&amp;rsquo;s corresponding sub-formula. Then, the subroutine Sampler is invoked that returns s uniformly and independently drawn samples using the properties of d-DNNF. Finally, KUS gives random assignment to the unassigned variables for each sample in the SampleList to account for unconstrained variables that do not appear in d-DNNF by invoking the subroutine RandomAssignment.&lt;/p&gt;
&lt;h3&gt;The Results&lt;/h3&gt;
Our experiments demonstrated that KUS outperformed both SPUR and UniGen2 state-of-the-art uniform and almost-uniform samplers by a factor of up to $3$ orders of magnitude in terms of runtime in some cases while achieving a geometric speedup of $1.7\times$ and $8.3\times$ over SPUR and UniGen2 respectively. The distribution generated by KUS is statistically indistinguishable from that generated by an ideal uniform sampler. Moreover, KUS is almost oblivious to the number of samples requested. Finally, we observe that KUS can benefit from different d-DNNF compilers, therefore suggesting development of portfolio samplers in future. One of the biggest advantage of KUS is in incremental sampling--fetching multiple, relatively small-sized samples, repeatedly. The typical use case of iterative sampling can be in repeated invocation of a sampling tool until the objective (such as desired coverage or violation of property) is achieved. In incremental-sampling KUS achieves speedups of upto 3 orders of magnitude.
&lt;h3&gt;Conclusion&lt;/h3&gt;
***
&lt;ul&gt;
&lt;li&gt;In this work, we have proposed a new approach for uniform sampling that builds on breakthrough progress in knowledge compilation&lt;/li&gt;
&lt;li&gt;Experimentally we have demonstrated that KUS outperformed state-of-the-art uniform and almost-uniform samplers&lt;/li&gt;
&lt;li&gt;We believe that the success of KUS will motivate researchers in verification and knowledge compilation communities to investigate a broader set of logical forms amenable to efficient uniform generation&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>WAPS: Weighted and Projected Sampling</title>
      <link>https://meelgroup.github.io/post/waps/</link>
      <pubDate>Fri, 03 May 2019 11:06:17 +0530</pubDate>
      <guid>https://meelgroup.github.io/post/waps/</guid>
      <description>&lt;p&gt;This blogpost talks about our tool 
&lt;a href=&#34;https://github.com/meelgroup/WAPS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WAPS&lt;/a&gt;. Specifically, we will talk about how we are able to utilize the idea of sampling using knowledge compilations (d-DNNFs) from our previous work (
&lt;a href=&#34;https://www.comp.nus.edu.sg/~meel/Papers/lpar18.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KUS&lt;/a&gt;) and generalize it in order to achieve weighted and projected sampling. You can read the paper 
&lt;a href=&#34;https://www.comp.nus.edu.sg/~meel/Papers/tacas19.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and get the tool 
&lt;a href=&#34;https://github.com/meelgroup/WAPS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. You can read the previous 
&lt;a href=&#34;https://meelgroup.github.io/post/kus/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog&lt;/a&gt; that describes uniform sampling using knowledge compilations, though it is not absolutely necessary for this post.&lt;/p&gt;
&lt;!-- Don&#39;t worry if you haven&#39;t read the previous blog, I am going to mention the required details here.  --&gt;
&lt;p&gt;Let&amp;rsquo;s talk about what it means to achieve weighted sampling at first.&lt;/p&gt;
&lt;h3&gt;Weighted Sampling&lt;/h3&gt;
***
Given a formula $F$ and a weight function $W$, the objective of $weighted$ sampling is to draw samples from the set of satisfying assignments of $F$ called $R\_{F}$ using a generator $\mathcal{G}^{w}(F, W)$ that ensures
 $$\forall y \in R_{F}, \mathsf{Pr}\left[\mathcal{G}^{w}(F, W) = y\right] = \frac{W(y)}{W(R\_F)}$$
&lt;!-- So, we are trying to construct a weighted probabilistic generator. --&gt;
&lt;p&gt;Intuitively, this just means that the probability of drawing a sample is proportional to its weight. In our case, we are dealing with literal-weighted weight function and the weight of an assignment is simply given by the product of weight of individual literals in the assignment. Broadly speaking, WAPS proceeds in three stages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;d-DNNF compilation&lt;/li&gt;
&lt;li&gt;Annotation&lt;/li&gt;
&lt;li&gt;Sampling.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let&amp;rsquo;s look at what a typical d-DNNF looks like:
&lt;img src=&#34;dDNNFexample.png&#34; alt=&#34;alt_text&#34;&gt;&lt;/p&gt;
&lt;p&gt;d-DNNF (Deterministic Decomposable Negation Normal Form) can be seen as a form of compact representation for the satisfying assignments of a given formula. One can also view it as the search space of component decomposition based DPLL procedures popularly employed in SAT solving and model counting. This perspective is helpful for Projected Sampling as you will see. Essentially, in a d-DNNF, the children of OR nodes have different (inconsistent to be precise) satisfying assignments (determinism); so, you can choose one of the children if you were to sample a satisfying assignment. On the other hand, the children of AND nodes are drawn over mutually disjoint sets of variables (decomposability); thus allowing you to simply stitch samples drawn from different children to get an overall sample.&lt;/p&gt;
&lt;p&gt;WAPS proceeds by first compiling the given CNF formula into its d-DNNF. This is followed by Annotation. The central idea in WAPS is to annotate the compiled d-DNNF in a way which allows weighted sampling by simply performing weighted bernoulli trials over d-DNNF in the Sampling phase (Refer to our 
&lt;a href=&#34;https://github.com/meelgroup/WAPS/raw/master/FullPaper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; for more details).
The weight annotation is summarised by the figures below:&lt;/p&gt;
&lt;div class=&#34;row&#34;&gt;
  &lt;div class=&#34;col-sm-5&#34;&gt;
    &lt;img src=&#34;WAnnotate.png&#34; alt=&#34;Snow&#34; class=&#34;center&#34;&gt;
  &lt;/div&gt;
  &lt;div class=&#34;col-sm-7&#34;&gt;
    &lt;img src=&#34;WAnnotate2.png&#34; alt=&#34;Forest&#34; class=&#34;center&#34;&gt;
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;In our paper, we show that this annotation scheme allows you to perform weighted sampling.&lt;/p&gt;
&lt;h3&gt; Weighted and Projected Sampling &lt;/h3&gt;
***
Given a formula $F$, a set of projecting variables $P$ and a weight function $W$, the objective of $weighted~and~projected$ sampling is to draw samples from the set of satisfying assignments of $F$ projected over $P$ called $R_{F\downarrow P}$ using a generator $\mathcal{G}^{wp}(F, P, W)$ that ensures
$$\forall y \in R\_{F\downarrow P}, \mathsf{Pr}\left[\mathcal{G}^{wp}(F, P, W) = y\right] = \frac{W(y)}{W(R\_{F\downarrow P})}$$
Intuitively, this means that samples drawn contain only a subset of variables ($P$) as opposed to all variables in the formula and these samples obey the weight distribution given by $W$ over the variables appearing in samples. This has applications in hardware verification and other places where encoding original problem into CNF generates additional Tseitin variables while weight distribution is only defined on original variables in the problem. In such cases, we are often interested in samples from variables of the original problem. 
&lt;h4&gt;Projected Sampling &lt;/h4&gt;
To achieve Projected Sampling, we aim to produce a d-DNNF which represents the set of satisfying assignments projected over a given set of projecting variables. To accomplish this, we modified Dsharp, a state of the art d-DNNF compiler to search first on projecting variables and then simply check if the residual formula is satisfiable to retain the corresponding path in d-DNNF. Notably, this technique has been used in the context of [Projected Model Counting](https://arxiv.org/abs/1507.07648) and [Quantitative Information Flow](https://link.springer.com/chapter/10.1007/978-3-642-40196-1_16) before. 
&lt;p&gt;The above technique combined with weighted sampling sums up the buildup of WAPS (Weighted and Projected Sampler).&lt;/p&gt;
&lt;h4&gt; Incremental Sampling &lt;/h4&gt;
Another interesting property as a side-effect of knowledge compilation based sampling is that incremental sampling (i.e. fetching multiple relatively short sized samples) can be performed efficiently. This is simply done by saving the compiled d-DNNF or its annotated version depending upon whether weights change in different iterations.
&lt;h3&gt; Results &lt;/h3&gt;
***
Our experiments demonstrate that WAPS is able to significantly outperform existing state-of-the-art weighted and projected sampler WeightGen, by up to $3$ orders of magnitude in terms of runtime while
achieving a geometric speedup of $296\times$. For $incremental~sampling$ i.e. fetching multiple, relatively small-sized samples, repeatedly, WAPS achieves a geometric speedup of $3.69$. Also, WAPS is almost oblivious to the number of samples requested. Empirically, the distribution generated by WAPS is statistically indistinguishable from that generated by an ideal weighted and projected sampler.  Also, while performing conditioned sampling in WAPS, we incur no extra cost in terms of runtime in most of the cases. Moreover, the performance of our knowledge compilation based sampling technique is found to be oblivious to weight distribution. Detailed data is available at [here](https://github.com/meelgroup/waps).
&lt;h3&gt; Final Thoughts &lt;/h3&gt;
***
This work has further tapped into the potential of sampling using knowledge compilations by developing procedures that allow weighted and projected sampling. We believe that the general idea of annotating knowledge compilations in different ways has even greater potential for sampling suited to a wider set of applications. Further work can also explore the development of faster sampling methods which leverage partially compiled d-DNNFs. Moreover, comparing the performance as well as functional capabilities with regard to sampling in different knowledge compilations such as SDDs(Sequential Decision Diagrams) is an interesting direction.
</description>
    </item>
    
    <item>
      <title>GANAK: A Scalable Probabilistic Exact Model Counter</title>
      <link>https://meelgroup.github.io/post/ganak/</link>
      <pubDate>Wed, 03 Apr 2019 11:06:17 +0530</pubDate>
      <guid>https://meelgroup.github.io/post/ganak/</guid>
      <description>&lt;p&gt;This blogpost talks about our tool 
&lt;a href=&#34;https://github.com/meelgroup/ganak&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GANAK&lt;/a&gt; that inherits current advancements in SAT solving and model counting, improves upon them and contributes new ideas, thereby outperforming state-of-the-art model counters. The source code of GANAK is available 
&lt;a href=&#34;https://github.com/meelgroup/ganak&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and the paper is available 
&lt;a href=&#34;https://www.comp.nus.edu.sg/~meel/Papers/ijcai19srsm.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;. The main result is that we can solve a lot more problems than before: &lt;img src=&#34;cactus.png&#34; alt=&#34;alt text&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s first define Model Counting.&lt;/p&gt;
&lt;h3&gt;Model Counting&lt;/h3&gt;
***
Given a Boolean formula $F$, over a set of variable $X$, model counting (aka \#SAT) seeks to compute the number of solutions of $F$. In 1979 Valiant showed that \#SAT is \#P-complete problem and in 1989 Toda proved that every problem in the polynomial hierarchy could be solved by just one call to a \#P oracle. Following types of model counting has been studied in the literature:
&lt;h4&gt;Exact Model Counting&lt;/h4&gt;
Given $F$, the problem of exact model counting is to compute the number of solutions of $F$.
&lt;h4&gt; Probabilistic Exact Model Counting &lt;/h4&gt;
Given $F$ and $\delta \in (0,1]$, probabilistic exact model counting estimates $\texttt{count}$ and guarantees that: $\mathsf{Pr}\big[|Solutions(F)| = \texttt{count}\big]\geq 1-\delta$. A recent study of different relaxations of model counting shows that probabilistic exact model counting is almost as hard as exact model counting
&lt;p&gt;Let&amp;rsquo;s see some of the applications of model counting&lt;/p&gt;
&lt;h3&gt; Applications of Model Counting &lt;/h3&gt;
***
Model counting is a fundamental problem with a wide variety of applications ranging from machine learning, quantified information flow, network reliability, planning, probabilistic reasoning, and many other related fields. For example, given a graph $G$ such that each of its edges fails with some probability and two nodes, $s$ and $t$, the problem of computing probability of existence of a path from $s$ to $t$ can be reduced to that of propositional model counting.
&lt;p&gt;Let&amp;rsquo;s talk about GANAK&lt;/p&gt;
&lt;h3&gt; GANAK &lt;/h3&gt;
***
GANAK is a scalable probabilistic exact model counter that inherits the strength of a state-of-the-art exact model counter, sharpSAT, and is equipped with the following new algorithmic advances
&lt;ul&gt;
&lt;li&gt;Probabilistic component caching (PCC)&lt;/li&gt;
&lt;li&gt;New variable branching heuristic (CSVSADS)&lt;/li&gt;
&lt;li&gt;New phase selection heuristic (PC)&lt;/li&gt;
&lt;li&gt;Independent support heuristic (IS)&lt;/li&gt;
&lt;li&gt;Exponentially decaying randomness heuristic (EDR)&lt;/li&gt;
&lt;li&gt;Learn and start over heuristic (LSO)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Detailed discussion about each of these heuristic can be found 
&lt;a href=&#34;https://www.comp.nus.edu.sg/~meel/Papers/ijcai19srsm.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt; Results &lt;/h3&gt;
***
&lt;p&gt;We evaluate the runtime performance of GANAK on $2031$ publicly available benchmarks arising from a wide range of applications of model counting. Our experiments demonstrate that GANAK performs best when all the heuristics describe in the previous section (except EDR) are enabled. GANAK outperforms state-of-the-art exact model counter, both in terms of PAR-2 score and the number of instances solved. Finally, in our experiments, the model count returned by GANAK was equal to the exact model count for all the benchmarks.&lt;/p&gt;
&lt;p&gt;We are thankful to the 
&lt;a href=&#34;https://www.nscc.sg/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;National Supercomputing Center Singapore&lt;/a&gt; for providing us computational resources to run our experiments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bosphorus: An ANF and CNF simplifier and converter</title>
      <link>https://meelgroup.github.io/post/bosphorus/</link>
      <pubDate>Sat, 05 Jan 2019 11:06:17 +0530</pubDate>
      <guid>https://meelgroup.github.io/post/bosphorus/</guid>
      <description>&lt;p&gt;We are happy to release our ANF and CNF simplifier and converter called &lt;a href=&#34;https://github.com/meelgroup/bosphorus&#34;&gt;Bosphorus&lt;/a&gt;. It has helped us break multiple real-world ciphers. It has been re-released with major work by Davin Choo &amp;amp; Kian Ming A. Chai from &lt;a href=&#34;https://www.dso.org.sg/&#34;&gt;DSO National Laboratories&lt;/a&gt; Singapore and Mate Soos &amp;amp; Kuldeep Meel from NUS. The &lt;a href=&#34;https://www.comp.nus.edu.sg/~meel/Papers/date-cscm19.pdf&#34;&gt;paper&lt;/a&gt; will be published at the &lt;a href=&#34;https://www.date-conference.com/&#34;&gt;DATE 2019&lt;/a&gt; conference.&lt;/p&gt;
&lt;h3&gt;ANFs and CNFs&lt;/h3&gt;
&lt;p&gt;Algebraic Normal Form is a form that is used by most cryptographers to describe symmetric ciphers, hash algorithms, and lately a lot of &lt;a href=&#34;https://csrc.nist.gov/projects/post-quantum-cryptography&#34;&gt;post-quantum asymmetric ciphers&lt;/a&gt;. It’s a very simple notation that basically looks like this:&lt;/p&gt;
&lt;pre class=&#34;wp-block-preformatted&#34;&gt;x1 ⊕ x2 ⊕ x3 = 0&lt;br&gt;x1 * x2 ⊕ x2 * x3 + 1 = 0&lt;/pre&gt;
&lt;p&gt;Where “⊕” represents XOR and “&lt;em&gt;” represents the AND operator. So the first line here is an XOR of binary variables x1, x2 and x3 and their XOR must be equal to 0. The second line means that “(x1 AND x2)  XOR (x2 AND x3)” must be equal to 1. This normal form allows to see a bunch of interesting things. For example, it allows us to see the so-called “maximum degree” of the set of equations, where the degree is the maximum number of variables AND-ed together in one line. The above set of equations has a maximum degree of 2, as (x1&lt;/em&gt;x2) is of degree 2. Degrees can often be a good indicator for the complexity of a problem.&lt;/p&gt;
&lt;p&gt;What’s good about ANFs is that there are a number of well-known algorithms to break problems described in them. For example, one can do &lt;a href=&#34;https://link.springer.com/content/pdf/10.1007%2F3-540-48405-1_2.pdf&#34;&gt;(re)linearization&lt;/a&gt; and Gauss-Jordan elimination, or one could run Grobner-basis algorithms such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Faug%C3%A8re%27s_F4_and_F5_algorithms&#34;&gt;F4/F5&lt;/a&gt; on it. Sometimes, the ANFs can also be solved by converting them to another normal form, Conjunctive Normal Form (CNF), used by SAT solvers. The CNF normal form looks like:&lt;/p&gt;
&lt;pre class=&#34;wp-block-preformatted&#34;&gt;x1 V x2 V x3&lt;br&gt;-x1 V x3&lt;/pre&gt;
&lt;p&gt;Where x1, x2 and x3 are binary variables, “V” is the logical OR, and each line must be equal to TRUE. Using CNF is interesting, because the solvers used to solve them, &lt;a href=&#34;https://en.wikipedia.org/wiki/Boolean_satisfiability_problem&#34;&gt;SAT solvers&lt;/a&gt;, typically provide a different set of trade-offs for solving than ANF problem solvers. SAT solvers tend to use more CPU time but a lot less memory, sometimes making problems viable to solve in the “real world”. Whereas sometimes breaking of a cipher is enough to be demonstrated on paper, it also happens that one wants to break a cipher in the &lt;a href=&#34;https://twitter.com/David3141593/status/1080606827384131590&#34;&gt;real world&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Bridging and Simplifying&lt;/h3&gt;
&lt;p&gt;We believe that Bosphorus is a first of its kind system that allows ANFs to be simplified using both CNF- and ANF-based systems. It can also convert between the two normal forms and can act both as an ANF and a CNF preprocessor, like &lt;a href=&#34;http://fmv.jku.at/papers/EenBiere-SAT05.pdf&#34;&gt;SatELite&lt;/a&gt; (by Een and Biere) was for CNF. We believe this makes Bosphorus unique and also uniquely useful, especially while working on ANFs.&lt;/p&gt;
&lt;p&gt;Bosphorus uses an iterative architecture that performs the following set of steps, either until it runs out of time or until fixed point:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Replace variables and propagate constants in the ANF &lt;/li&gt;&lt;li&gt;Run limited &lt;a href=&#34;https://en.wikipedia.org/wiki/XSL_attack&#34;&gt;Extended Linarization (XL)&lt;/a&gt;  and inject back unit and binary XORs&lt;/li&gt;&lt;li&gt;Run limited &lt;a href=&#34;https://rd.springer.com/content/pdf/10.1007%2F978-3-642-34047-5_18.pdf&#34;&gt;ElimLin&lt;/a&gt;  and inject back unit and binary XORs&lt;/li&gt;&lt;li&gt;Convert to CNF, run a SAT solver for a limited number of conflicts and inject back unit and binary (and potentially longer) XORs&lt;/li&gt;&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;Bosphorus.png&#34; alt=&#34;alt_text&#34;&gt;&lt;/p&gt;
&lt;p&gt;In other words, the system is an iterative simplifier/preprocessor that invokes multiple reasoning systems to try to simplify the problem as much as possible. It can outright solve the system, as most of these reasoning systems are complete, but the point is to run them only to a certain limit and inject back into the ANF the easily “digestible” information. The simplified ANF can then either be output as an ANF or a CNF.&lt;/p&gt;
&lt;p&gt;Bosphorus can also take a CNF as input, perform the trivial transformation of it to ANF and then treat it as an ANF. This allows the CNF to be simplified using techniques previously unavailable to CNF systems, such as XL.&lt;/p&gt;
&lt;h3&gt;ANF to CNF Conversion&lt;/h3&gt;
&lt;p&gt;ANF-to-CNF conversion is not considered that hard, and that’s why there hasn’t been too much academic effort devoted to it. However, it’s an important step without which a lot of opportunities would be missed.&lt;/p&gt;
&lt;p&gt;The implemented system contains a pretty advanced ANF-to-CNF converter, using Karnaugh tables through &lt;a href=&#34;https://en.wikipedia.org/wiki/Espresso_heuristic_logic_minimizer&#34;&gt;Espresso&lt;/a&gt;, XOR cutting, monomial reuse, etc. It should give a pretty optimal CNF for all ANFs. So Bosphorus can be used also just as an ANF-to-CNF converter, though it’s so much more.&lt;/p&gt;
&lt;h3&gt;Final Thoughts&lt;/h3&gt;
&lt;p&gt;One of the biggest capabilities of &lt;a href=&#34;https://github.com/meelgroup/bosphorus&#34;&gt;Bosphorus&lt;/a&gt; is that it can simplify/preprocess ANF systems so more heavyweight ANF solvers can have a go at them. Its ANF simplification is so powerful, it can even help to solve some CNFs by lifting them to ANF, running the ANF simplifiers, converting it back to CNF, and solving that(!). We believe our initial results, published in the &lt;a href=&#34;https://www.comp.nus.edu.sg/~meel/Papers/date-cscm19.pdf&#34;&gt;paper&lt;/a&gt;, are very encouraging. Further, the system is in a ready-to-use state: there is a &lt;a href=&#34;https://cloud.docker.com/repository/docker/msoos/bosphorus&#34;&gt;Docker image&lt;/a&gt;, the source should build without a hitch, and there is even a precompiled Linux &lt;a href=&#34;https://github.com/meelgroup/bosphorus/releases/&#34;&gt;binary&lt;/a&gt;. We would love to hear about your experience using it.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;Thanks to Karsten Nohl from &lt;a href=&#34;https://srlabs.de/&#34;&gt;Security Research Labs.&lt;/a&gt;&lt;/i&gt; This post has been adapted from &lt;a href=&#34;https://www.msoos.org/2019/01/bosphorus-an-anf-and-cnf-simplifier-and-converter/&#34;&gt;Mate&amp;rsquo;s post&lt;/a&gt; on Bosphorus.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ApproxMCv3: A modern approximate model counter</title>
      <link>https://meelgroup.github.io/post/approxmcv3/</link>
      <pubDate>Sun, 25 Nov 2018 11:06:17 +0530</pubDate>
      <guid>https://meelgroup.github.io/post/approxmcv3/</guid>
      <description>&lt;p&gt;ApproxMC is a scalable, approximate model counter that provides PAC (probably approximately correct) guarantees. We have been working very hard on speeding up approximate model counting for SAT and have made real progress. The research paper, accepted at AAAI-19 is available &lt;a href=&#34;https://www.comp.nus.edu.sg/~meel/Papers/aaai19-sm.pdf&#34;&gt;here&lt;/a&gt;. The code is available &lt;a href=&#34;https://github.com/meelgroup/ApproxMC&#34;&gt;here&lt;/a&gt; (release with static binary &lt;a href=&#34;https://github.com/meelgroup/ApproxMC/releases&#34;&gt;here&lt;/a&gt;). The main result is that we can solve a &lt;strong&gt;lot&lt;/strong&gt; more problems than before. The speed of solving is orders(!) of magnitude faster than the previous best system:&lt;/p&gt;
&lt;p&gt;&lt;img class=&#34;size-full wp-image-3214 aligncenter&#34; src=&#34;http://www.msoos.org/wordpress/wp-content/uploads/2018/11/Screenshot_20181125_202611.png&#34; alt=&#34;&#34; width=&#34;1202&#34; height=&#34;740&#34; srcset=&#34;https://www.msoos.org/wordpress/wp-content/uploads/2018/11/Screenshot_20181125_202611.png 1202w, https://www.msoos.org/wordpress/wp-content/uploads/2018/11/Screenshot_20181125_202611-300x185.png 300w, https://www.msoos.org/wordpress/wp-content/uploads/2018/11/Screenshot_20181125_202611-768x473.png 768w, https://www.msoos.org/wordpress/wp-content/uploads/2018/11/Screenshot_20181125_202611-1024x630.png 1024w&#34; sizes=&#34;(max-width: 1202px) 100vw, 1202px&#34; /&gt;&lt;/p&gt;
&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;The idea of approximate model counting, originally &lt;a href=&#34;https://arxiv.org/pdf/1306.5726&#34;&gt;by Chakraborty, Meel and Vardi&lt;/a&gt; was a huge hit back in 2013, and many papers have followed it, trying to improve its results. All of them were basically tied to &lt;a href=&#34;https://github.com/msoos/cryptominisat&#34;&gt;CryptoMiniSat&lt;/a&gt;, the SAT solver that is maintained by Mate, as all of them relied on XOR constraints being added to the regular CNF of a typical SAT problem.&lt;/p&gt;
&lt;p&gt;So it made sense to examine what CryptoMiniSat could do to improve the speed of approximate counting. This time interestingly coincided with the removal of XORs in CryptoMiniSat. The problem was the following: A lot of new in- and preprocessing systems were being invented, mostly by Armin Biere et al, and they couldn&#39;t be added to CryptoMiniSat, because they didn&amp;#8217;t take into account XOR constraints. They handled CNF just fine, but not XORs. So XORs became a burden, and they were removed in versions 3 and 4 of CryptoMiniSat. But there was need, and this being an exciting area, the XORs had to come back.&lt;/p&gt;
&lt;h3&gt;Blast-Inprocess-Recover-Destroy&lt;/h3&gt;
&lt;p&gt;But how to both have and not have XOR constraints? Re-inventing all the algorithms for XORs was not a viable option. The solution we came up with was a rather trivial one: forget the XORs during inprocessing and recover them after. The CNF would always remain the source of truth. Extracting all the XORs after in- and preprocessing would allow us to run the Gauss-Jordan elimination on the XORs post-recovery.&lt;/p&gt;
&lt;p&gt;The process is conceptually quite easy:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Blast&lt;/strong&gt; all XORs into clauses that are in the input using intermediate variables.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Perform&lt;/strong&gt; &lt;strong&gt;pre- or inprocessing&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Recover the XORs&lt;/strong&gt; from the CNF.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Run the CDCL and Gauss-Jordan&lt;/strong&gt; code at the same time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Destroy the XORs&lt;/strong&gt; and goto 2.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This system allows for everything to be in CNF form, lifting the XORs out when necessary and then forgetting them when it&amp;#8217;s convenient. All of these steps are rather trivial, except &lt;b&gt;recovery&lt;/b&gt;, as explained below.&lt;/p&gt;
&lt;h3&gt;XOR recovery&lt;/h3&gt;
&lt;p&gt;Recovering XORs sounds like a trivial task. Let&amp;#8217;s say we have the following clauses&lt;/p&gt;
&lt;pre class=&#34;wp-block-preformatted&#34;&gt;
 x1 V  x2 V  x3
-x1 V -x2 V  x3
 x1 V -x2 V -x3
-x1 V  x2 V -x3
&lt;/pre&gt;
&lt;p&gt;This is conceptually equivalent to the XOR v1+v2+v3=1. So recovering this is trivial, and has been done before, by Heule in particular, in his &lt;a href=&#34;http://www.st.ewi.tudelft.nl/sat/theses/heule_phd.pdf&#34;&gt;PhD thesis&lt;/a&gt;. The issue with the above is the following: a stronger system than the above still implies the XOR, but doesn&amp;#8217;t look the same. For example:&lt;/p&gt;
&lt;pre class=&#34;wp-block-preformatted&#34;&gt;
 x1 V  x2 V  x3
-x1 V -x2 V  x3
 x1 V -x2 V -x3
-x1 V  x2
&lt;/pre&gt;
&lt;p&gt;This is almost equivalent to the previous set of clauses, but misses a literal from one of the clauses. It still implies the XOR of course. Now what? And what to do when missing literals mean that an entire clause can be missing? The algorithm to recover XORs in such cases is non-trivial. It&amp;#8217;s non-trivial not only because of the complexity of how many combinations of missing literals and clauses there can be (it&amp;#8217;s exponential) but because one must do this work extremely fast because SAT solvers are sensitive to time.&lt;/p&gt;
&lt;p&gt;The algorithm that is in the &lt;a href=&#34;https://www.comp.nus.edu.sg/~meel/Papers/aaai19-sm.pdf&#34;&gt;paper&lt;/a&gt; explains all the bit-fiddling and cache-friendly data layout used along with some fun algorithms. We even managed to use compiler intrinsics to use target-specific assembly instructions for hamming weight calculation.&lt;/p&gt;
&lt;h3&gt;The results&lt;/h3&gt;
&lt;p&gt;The results, as shown above, speak for themselves. Problems that took thousands of seconds to solve can now be solved under 20. The reason for such incredible speedup is basically the following: CryptoMiniSatv2 was way too clunky and didn&amp;#8217;t have all the fun stuff that CryptoMiniSatv5 has, plus the XOR handling was incorrect, loosing XORs and the like. The published algorithm solves the underlying issue and allows CNF pre- and inprocessing to happen independent of XORs, thus enabling CryptoMiniSatv5 to be used in all its glory. And CryptoMiniSatv5 is &lt;em&gt;fast,&lt;/em&gt; as per the this year&amp;#8217;s SAT Competition &lt;a href=&#34;http://sat2018.forsyte.tuwien.ac.at/index.php?cat=results&#34;&gt;results&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;i&gt;We thank the &lt;a href=&#34;https://www.nscc.sg/&#34;&gt;National Supercomputing Center Singapore&lt;/a&gt;  that allowed us to run a large number of benchmarks on their machines, using at least 200 thousand CPU hours to make this paper. &lt;/i&gt;This post has been adapted from &lt;a href=&#34;https://www.msoos.org/2018/11/approxmcv3-a-modern-approximate-model-counter/&#34;&gt;Mate&#39;s post&lt;/a&gt; on ApproxMCv3.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
